{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "G3N7k27OUM9E"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import ParamSpecArgs\n",
        "from jax._src.api import jit\n",
        "from sklearn.utils import shuffle\n",
        "from scipy import signal\n",
        "from google.colab import files\n",
        "\n",
        "import jax.numpy as jnp, random\n",
        "import numpy as np\n",
        "import os, shutil\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AQg_T4tMmJYM"
      },
      "outputs": [],
      "source": [
        "\"\"\"Creating the Layers for the CNN\"\"\"\n",
        "\n",
        "class Layer:\n",
        "  def __init__(self):\n",
        "    self.input = None\n",
        "    self.output = None\n",
        "\n",
        "  def forward(self, input):\n",
        "    pass\n",
        "  \n",
        "  def backward(self, o_grad, lr):\n",
        "    \"\"\"\n",
        "      where o_grad = output gradient\n",
        "            lr = learning rate\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "class Activation(Layer):\n",
        "    def __init__(self, activation, activation_prime):\n",
        "        self.activation = activation\n",
        "        self.activation_prime = activation_prime\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        return self.activation(self.input)\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        return np.multiply(output_gradient, self.activation_prime(self.input))\n",
        "\n",
        "\n",
        "class Convolutional(Layer):\n",
        "  def __init__(self, input_shape, kernel_size, depth):\n",
        "    self.input_depth, self.input_width, self.input_height = input_shape\n",
        "    self.depth = depth\n",
        "    self.input_shape = input_shape\n",
        "\n",
        "    \"\"\"\n",
        "      y = x - K + 1\n",
        "    \"\"\"\n",
        "    self.output_shape = (depth, self.input_height - kernel_size + 1, self.input_width - kernel_size + 1)\n",
        "    self.kernels_shape = (depth, self.input_depth, kernel_size, kernel_size)\n",
        "    self.kernels = np.random.normal(*self.kernels_shape)\n",
        "    self.biases = np.random.normal(*self.output_shape)\n",
        "  \n",
        "  def forward(self, input):\n",
        "    self.input = input\n",
        "    self.output = np.copy(self.biases)\n",
        "    \n",
        "    \"\"\"\n",
        "      output(y) = Bias(b) + Sum( Input( x ) * Kernel( K ) )\n",
        "      where: * = Cross Correlation\n",
        "    \n",
        "    \"\"\"\n",
        "    for i in range(self.depth):\n",
        "      for j in range(self.input_depth):\n",
        "        self.output[i] += signal.correlate2d(self.input[j], self.kernels[i,j], \"valid\")\n",
        "    \n",
        "    return self.output\n",
        "  \n",
        "  def backward(self, o_grad, lr):\n",
        "    #update params and return input grad\n",
        "\n",
        "    kernels_gradient = np.zeros(self.kernels_shape)\n",
        "    input_gradient = np.zeros(self.input_shape)\n",
        "\n",
        "    for i in range(self.depth):\n",
        "      for j in range(self.input_depth):\n",
        "        kernels_gradient[ i , j ] = signal.correlate2d(self.input[j], o_grad[i], \"valid\")\n",
        "        input_gradient[ j ] += signal.convolve2d(o_grad[i], self.kernels[ i , j ], \"full\")\n",
        "\n",
        "    self.kernels -= lr * kernels_gradient\n",
        "    self.biases -= lr * o_grad\n",
        "\n",
        "    return input_gradient\n",
        "\n",
        "\n",
        "class Convolutional_2(Layer):\n",
        "  def __init__(self, num_filters, filter_size):\n",
        "    self.num_filters = num_filters\n",
        "    self.filter_size = filter_size\n",
        "    self.conv_filter = np.random.randn(num_filters, filter_size, filter_size) / (filter_size * filter_size)\n",
        "\n",
        "  def image_region(self, image):\n",
        "    height, width = image.shape\n",
        "    self.image = image\n",
        "    for j in range(height - self.filter_size + 1):\n",
        "      for k in range(width - self.filter_size + 1):\n",
        "        image_patch = image [j : (j+self.filter_size), k:( k + self.filter_size)]\n",
        "        yield image_patch, j, k\n",
        "  \n",
        "  def forward(self, images):\n",
        "    height, width = images.shape\n",
        "    self.image = images\n",
        "\n",
        "    conv_out = np.zeros((height - self.filter_size + 1 , width - self.filter_size + 1, self.num_filters))\n",
        "    for image_patch , i , j in self.image_region(images):\n",
        "      conv_out[i,j] = np.sum(image_patch * self.conv_filter, axis = (1,2))\n",
        "    return conv_out\n",
        "\n",
        "  def backward(self, dL_dout, lr):\n",
        "    dL_dF_params = np.zeros(self.conv_filter.shape)\n",
        "    for image_patch, i, j in self.image_region(self.images):\n",
        "      for k in range(self.num_filters):\n",
        "        dL_dF_params[k] += image_patch * dL_dout[i,j,k]\n",
        "    \n",
        "    #update params\n",
        "    self.conv_filter -= lr * dL_dF_params\n",
        "    return dL_dF_params\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Dense(Layer):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weights = np.random.randn(output_size, input_size)\n",
        "        self.bias = np.random.randn(output_size, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        return np.dot(self.weights, self.input) + self.bias\n",
        "\n",
        "    def backward(self, output_gradient, learning_rate):\n",
        "        weights_gradient = np.dot(output_gradient, self.input.T)\n",
        "        input_gradient = np.dot(self.weights.T, output_gradient)\n",
        "        self.weights -= learning_rate * weights_gradient\n",
        "        self.bias -= learning_rate * output_gradient\n",
        "        return input_gradient\n",
        "\n",
        "\"\"\"\n",
        "  Reshape Layer ==> the output of the Conv Layer is 3d and must be reshaped\n",
        "\"\"\"\n",
        "\n",
        "class Reshape(Layer):\n",
        "  def __init__(self, input_shape, output_shape):\n",
        "    self.input_shape = input_shape\n",
        "    self.output_shape = output_shape\n",
        "\n",
        "  def forward(self, input):\n",
        "    return np.reshape(input, self.output_shape)\n",
        "  \n",
        "  def backward(self, o_grad, lr):\n",
        "    return np.reshape(o_grad, self.input_shape)\n",
        "\n",
        "\"\"\"\n",
        "  For this task, i believe that RMSE would be the better error to use as opposed to\n",
        "  BCE (however i did provide an implementation of BCE just incase i am wrong here)\n",
        "\"\"\"\n",
        "\n",
        "class Losses():\n",
        "  def mse(y_true, y_pred):\n",
        "      return np.mean(np.power(y_true - y_pred, 2))\n",
        "\n",
        "  def mse_prime(y_true, y_pred):\n",
        "      return 2 * (y_pred - y_true) / np.size(y_true)\n",
        "\n",
        "  def binary_cross_entropy(y_true, y_pred):\n",
        "      return np.mean(-y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "  def binary_cross_entropy_prime(y_true, y_pred):\n",
        "      return ((1 - y_true) / (1 - y_pred) - y_true / y_pred) / np.size(y_true)\n",
        "\n",
        "\"\"\"\n",
        "  different types of activations. I think for this problem, i think i will use\n",
        "  Tanh activation function\n",
        "\"\"\"\n",
        "\n",
        "class Tanh(Activation):\n",
        "  def __init__(self):\n",
        "    \n",
        "    def tanh(x):\n",
        "      return np.tanh(x)\n",
        "    \n",
        "    def tanh_prime(x):\n",
        "      return 1 - np.tanh(x) ** 2\n",
        "    \n",
        "    super().__init__(tanh, tanh_prime)\n",
        "\n",
        "class Sigmoid(Activation):\n",
        "  def __init__(self):\n",
        "    def sigmoid(x):\n",
        "      return 1 / (1 + np.exp(-x))\n",
        "    \n",
        "    def sigmoid_prime(x):\n",
        "      s = sigmoid(x)\n",
        "\n",
        "      return s * (1-s)\n",
        "\n",
        "    super().__init__(sigmoid, sigmoid_prime)\n",
        "\n",
        "class Softmax(Layer):\n",
        "  def forward(self, input):\n",
        "    temp = np.exp(input)\n",
        "\n",
        "    self.output = temp/np.sum(temp)\n",
        "\n",
        "    return self.output\n",
        "\n",
        "  def backward(self, o_grad, lr):\n",
        "    n = np.size(self.output)\n",
        "\n",
        "    return np.dot((np.identity(n) - self.output.T) * self.output , o_grad)\n",
        "\n",
        "\n",
        "\n",
        "class Softmax_2(Layer):\n",
        "  def __init__(self, input_node, softmax_node):\n",
        "    self.w = np.random.randn(input_node, softmax_node) / input_node\n",
        "    self.b = np.zeros(softmax_node)\n",
        "\n",
        "  def forward(self, image):\n",
        "    self.orginialShape = image.shape\n",
        "    image_mod = image.flatten()\n",
        "    self.input_mod = image_mod\n",
        "\n",
        "    o_val = np.dot(image_mod, self.w) + self.b\n",
        "    \n",
        "    self.o_val = o_val\n",
        "\n",
        "    exp_out = np.exp(o_val)\n",
        "\n",
        "    return exp_out / np.sum(exp_out, axis = 0)\n",
        "  \n",
        "  def backward(self, o_grad, lr):\n",
        "    for i, grad in enumerate(o_grad):\n",
        "      if grad == 0: continue\n",
        "    \n",
        "      transformation_eq = np.exp(self.out)\n",
        "      S_total = np.sum(transformation_eq)\n",
        "\n",
        "      dy_dZ = -transformation_eq[i]*transformation_eq / (S_total **2)\n",
        "      dy_dZ[i] = transformation_eq[i] * (S_total - transformation_eq[i]) / (S_total ** 2)\n",
        "\n",
        "      dz_dW = self.input_mod\n",
        "      dz_dB = 1\n",
        "      dz_dX = self.w\n",
        "\n",
        "      dL_dz = grad * dy_dZ\n",
        "\n",
        "      dL_dW = dz_dW[np.newaxis].T @ dL_dz[np.newaxis]\n",
        "      dL_db = dL_dz * dz_dB\n",
        "      dL_dX = dz_dX @ dL_dz\n",
        "\n",
        "      self.w -= lr * dL_dW\n",
        "      self.b -= lr * dL_db\n",
        "\n",
        "      return dL_dX.reshape(self.originalShape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MaxPool(Layer):\n",
        "  def __init__(self, filter_size):\n",
        "    self.filter_size = filter_size\n",
        "\n",
        "  def image_region(self, image):\n",
        "    n_height = image.shape[0]\n",
        "    n_width = image.shape[1]\n",
        "\n",
        "    self.image = image\n",
        "    \n",
        "    for i in range(n_height):\n",
        "      for j in range(n_width):\n",
        "        image_patch = image[(i * self.filter_size) : ( i * self.filter_size + self.filter_size), (j * self.filter_size) : (j * self.filter_size) : ( j * self.filter_size + self.filter_size)]\n",
        "        yield image_patch, i, j \n",
        "  \n",
        "  def forward(self, image):\n",
        "    h ,w ,num_filters = image.shape\n",
        "    output = np.zeros((h // self.filter_size, w // self.filter_size, num_filters))\n",
        "\n",
        "    for image_patch, i , j in self.image_region(image):\n",
        "      output[i,j] = np.amax(image_patch, axis = (0,1))\n",
        "    return output\n",
        "\n",
        "  def backward(self, o_grad):\n",
        "    o_grad_maxpool= np.zeros(self.image.shape)\n",
        "\n",
        "    for image_patch, i , j in self.image_region(self.image):\n",
        "      h, w, num_filters = image_patch.shape\n",
        "      max_val = np.amax(image_patch, axis = (0,1))\n",
        "\n",
        "      for il in range(h):\n",
        "        for jl in range(w):\n",
        "          for kl in range(num_filters):\n",
        "            if image_patch(i,j,k) == max_val:\n",
        "              o_grad_maxpool[i * self.filter_size + il, j * self.filter_size + jl, kl] = o_grad[i,j,kl]\n",
        "    \n",
        "    return o_grad_maxpool\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9Q2F8tQSPc7"
      },
      "source": [
        "#Preprocessing kaggle data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": "OK"
            }
          }
        },
        "id": "aDBfwKNQjqQ6",
        "outputId": "cdb74cd3-f6fe-40f4-dc14-b363abf36d12"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4d5c0b85-e224-49db-8e5e-9cda4c72f631\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4d5c0b85-e224-49db-8e5e-9cda4c72f631\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\"\"\"import kaggle.json here\"\"\"\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rL3QQbISLbI"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('/root/.kaggle/'):\n",
        "  os.mkdir('/root/.kaggle/')\n",
        "  \n",
        "!mv /content/kaggle.json /content/sample_data/\n",
        "!mv /content/sample_data/kaggle.json /root/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92S-sEGx7II-"
      },
      "outputs": [],
      "source": [
        "%ls /root/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Gu3sSCD7NJQ"
      },
      "outputs": [],
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqzn_FTU7ThT"
      },
      "outputs": [],
      "source": [
        "!kaggle competitions files -c facial-keypoints-detection\n",
        "!kaggle competitions download -c facial-keypoints-detection -p /content/kaggle/facial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEhjoHVP7tz6"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/kaggle/facial/facial-keypoints-detection.zip -d /content/kaggle/facial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wvia41zJ-pEa"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('/content/kaggle/facial/data'):\n",
        "  os.mkdir('/content/kaggle/facial/data')\n",
        "\n",
        "!unzip -q /content/kaggle/facial/training.zip -d  /content/kaggle/facial/data\n",
        "!unzip -q /content/kaggle/facial/test.zip -d /content/kaggle/facial/data\n",
        "!unzip -q /content/kaggle/facial/facial-keypoints-detection.zip -d /content/kaggle/facial/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7au6doOETp9"
      },
      "outputs": [],
      "source": [
        "idlookup_df = pd.read_csv('/content/kaggle/facial/data/IdLookupTable.csv')\n",
        "train_df = pd.read_csv('/content/kaggle/facial/data/training.csv')\n",
        "test_df = pd.read_csv('/content/kaggle/facial/data/test.csv')\n",
        "\n",
        "idlookup_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7Dt_mhjGaEq"
      },
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ds8Cm2uGcsr"
      },
      "outputs": [],
      "source": [
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3N5qxDHGi4R"
      },
      "outputs": [],
      "source": [
        "train_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-6Bq1W6GlxZ"
      },
      "outputs": [],
      "source": [
        "train_df.duplicated().sum()\n",
        "print()\n",
        "train_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hB6opj30G11v"
      },
      "outputs": [],
      "source": [
        "unclean_train_data = train_df.fillna(method = \"ffill\")\n",
        "print(f\"unclean_train_data: {np.shape(unclean_train_data)}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T96t3rtFH9a8"
      },
      "outputs": [],
      "source": [
        "train_df.dropna(inplace=True)\n",
        "train_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExbPcz1oIH8v"
      },
      "outputs": [],
      "source": [
        "print(\"Number of missing pixel values: {}\".format(len(unclean_train_data) - int(train_df.Image.apply(lambda x: len(x.split())).value_counts().values)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBUuNDD8I5rJ"
      },
      "outputs": [],
      "source": [
        "img_height = 96\n",
        "img_width = 96\n",
        "\n",
        "img_h = 96\n",
        "img_w = 96\n",
        "\n",
        "def plot_sample(image, keypoint, axis, title):\n",
        "    image = image.reshape(img_height, img_width)\n",
        "    axis.imshow(image, cmap=\"gray\")\n",
        "    axis.scatter(keypoint[::2], keypoint[1::2], marker='x', s=20)\n",
        "    plt.title(title)\n",
        "\n",
        "\n",
        "def convert_data_to_image(image_data):\n",
        "    images = []\n",
        "    for _, sample in image_data.iterrows():\n",
        "        image = np.array(sample[\"Image\"].split(' '), dtype=int)\n",
        "        image = np.reshape(image, (img_height,img_width,1))\n",
        "        images.append(image)\n",
        "    images = np.array(images)/255\n",
        "    return images\n",
        "\n",
        "def get_keypoints_features(keypoint_data):\n",
        "    keypoint_data = keypoint_data.drop(\"Image\", axis=1)\n",
        "    keypoint_features = []\n",
        "    for _, sample_keypoints in keypoint_data.iterrows():\n",
        "        keypoint_features.append(sample_keypoints)\n",
        "    \n",
        "    keypoint_features = np.array(keypoint_features, dtype=\"float\")\n",
        "    return keypoint_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtJZLZE8Qsqs"
      },
      "outputs": [],
      "source": [
        "sample_image_index = 10\n",
        "\n",
        "train_images = convert_data_to_image(train_df)\n",
        "\n",
        "train_keypoints = get_keypoints_features(train_df)\n",
        "\n",
        "print(\"Shape of train_images: {}\".format(np.shape(train_images)))\n",
        "print(\"Shape of train_keypoints: {}\".format(np.shape(train_keypoints)))\n",
        "\n",
        "fig, axis = plt.subplots()\n",
        "plot_sample(train_images[sample_image_index], train_keypoints[sample_image_index], axis, \"Sample image & keypoints\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8yBZbvGVl-5"
      },
      "outputs": [],
      "source": [
        "sample_image_index = 10\n",
        "\n",
        "unclean_train_images = convert_data_to_image(unclean_train_data)\n",
        "\n",
        "unclean_train_keypoints = get_keypoints_features(unclean_train_data)\n",
        "\n",
        "print(f\"Shape of unclean train images: {np.shape(unclean_train_images)}\")\n",
        "print(f\"Shape of unclean train keypoints: {np.shape(unclean_train_keypoints)}\")\n",
        "\n",
        "fig, axis = plt.subplots()\n",
        "plot_sample(unclean_train_images[sample_image_index], unclean_train_keypoints[sample_image_index], axis, \"Sample image and keypoints\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Y52PzqGYud9"
      },
      "outputs": [],
      "source": [
        "test_images = convert_data_to_image(test_df)\n",
        "\n",
        "test_keypoints = get_keypoints_features(test_df)\n",
        "\n",
        "print(f\"Shape of test_images: {np.shape(test_images)}\")\n",
        "print(f\"Shape of test_keypoints: {np.shape(test_keypoints)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EPPWOTPacaO"
      },
      "outputs": [],
      "source": [
        "full_train_images = train_images\n",
        "\n",
        "full_train_keypoints = train_keypoints\n",
        "\n",
        "full_train_images = np.concatenate((full_train_images, unclean_train_images))\n",
        "full_train_keypoints = np.concatenate((full_train_keypoints, unclean_train_keypoints))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCRBecCRbO6Q"
      },
      "outputs": [],
      "source": [
        "def left_right_flip(images, keypoint_features):\n",
        "  flipped = []\n",
        "  flipped_images = np.flip(images, axis=2)\n",
        "\n",
        "  for i, sample_keypoints in enumerate(keypoint_features):\n",
        "    flipped.append([96.-coord if i%2==0 else coord for index,coord in enumerate(sample_keypoints)])\n",
        "  \n",
        "  return flipped_images, flipped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmPMSO9Htstc"
      },
      "outputs": [],
      "source": [
        "horizontal_flip = True\n",
        "\n",
        "flipped_train_images = np.array([])\n",
        "flipped_train_keypoints = np.array([])\n",
        "\n",
        "if horizontal_flip:\n",
        "    flipped_train_images, flipped_train_keypoints = left_right_flip(train_images, train_keypoints)\n",
        "    print(\"Shape of flipped_train_images: {}\".format(np.shape(flipped_train_images)))\n",
        "    print(\"Shape of flipped_train_keypoints: {}\".format(np.shape(flipped_train_keypoints)))\n",
        "    \n",
        "    full_train_images = np.concatenate((full_train_images, flipped_train_images))\n",
        "    full_train_keypoints = np.concatenate((full_train_keypoints, flipped_train_keypoints))\n",
        "    \n",
        "    fig, axis = plt.subplots()\n",
        "    plot_sample(flipped_train_images[sample_image_index], flipped_train_keypoints[sample_image_index], axis, \"Horizontally Flipped\")\n",
        "    \n",
        "    print(full_train_images.shape)\n",
        "    print(full_train_keypoints.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ek82BTV2fts"
      },
      "outputs": [],
      "source": [
        "def rotate_aug(images, keypoints_features, rotation_angles):\n",
        "    rotated_images = []\n",
        "    rotated_keypoints_features = []\n",
        "    for angle in rotation_angles:    \n",
        "        for angle in [angle,-angle]:\n",
        "            M = cv2.getRotationMatrix2D((img_height/2,img_width/2), angle, 1.0)\n",
        "            angle_rad = -angle*math.pi/180\n",
        "            \n",
        "            \n",
        "            for image in images:\n",
        "                rotated_image = cv2.warpAffine(image, M, (img_height,img_width), flags=cv2.INTER_CUBIC)\n",
        "                rotated_images.append(rotated_image)\n",
        "\n",
        "            for keypoint in keypoints_features:\n",
        "                rotated_keypoint = keypoint - img_height/2.    \n",
        "                \n",
        "                for idx in range(0,len(rotated_keypoint),2):\n",
        "                    rotated_keypoint[idx]=(rotated_keypoint[idx]*math.cos(angle_rad)-rotated_keypoint[idx+1]*math.sin(angle_rad))\n",
        "                    rotated_keypoint[idx+1]=(rotated_keypoint[idx]*math.sin(angle_rad)+rotated_keypoint[idx+1]*math.cos(angle_rad))\n",
        "                    \n",
        "                rotated_keypoint += img_height/2\n",
        "                rotated_keypoints_features.append(rotated_keypoint)\n",
        "            \n",
        "    return np.reshape(rotated_images,(-1,img_height,img_width,1)), rotated_keypoints_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_IgUUC79i4d"
      },
      "outputs": [],
      "source": [
        "rotation_augmentation = True\n",
        "\n",
        "rot_angles = [ 15 ]\n",
        "\n",
        "rotated_T_images = np.array([])\n",
        "rotated_T_keypoints = np.array([])\n",
        "\n",
        "if rotation_augmentation:\n",
        "  rotated_train_images, rotated_train_keypoints_features = rotate_aug(train_images, train_keypoints, rot_angles)\n",
        "\n",
        "  print(f'Shape of rotated train images: {np.shape(rotated_T_images)}')\n",
        "  print(f'Shape of rotated train keypoints: {np.shape(rotated_T_keypoints)}')\n",
        "\n",
        "  full_train_images = np.concatenate((full_train_images, rotated_train_images))\n",
        "  full_train_keypoints = np.concatenate((full_train_keypoints, rotated_train_keypoints_features))\n",
        "\n",
        "  fig, axis = plt.subplots()\n",
        "  plot_sample(rotated_train_images[sample_image_index], rotated_train_keypoints_features[sample_image_index], axis, \"Rotation Augmentation\")\n",
        "    \n",
        "  print(full_train_images.shape)\n",
        "  print(full_train_keypoints.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgxrkDrBbTGk"
      },
      "outputs": [],
      "source": [
        "def alter_brightness(images, keypoints):\n",
        "  altered_brightness_images = []\n",
        "\n",
        "  inc_brightness_images = np.clip(images*1.2, 0.0, 1.0)\n",
        "  dec_bgitheness_images = np.clip(images*0.6, 0.0, 1.0)\n",
        "\n",
        "  altered_brightness_images.extend(inc_brightness_images)\n",
        "  altered_brightness_images.extend(dec_bgitheness_images)\n",
        "\n",
        "  return altered_brightness_images, np.concatenate((keypoints, keypoints))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aB7IrGbIcWmh"
      },
      "outputs": [],
      "source": [
        "brightness_augmentation = True\n",
        "\n",
        "altered_brightness_train_images = np.array([])\n",
        "altered_brightness_train_keypoints = np.array([])\n",
        "\n",
        "if brightness_augmentation:\n",
        "  altered_brightness_train_images, altered_brightness_train_keypoints = alter_brightness(train_images, train_keypoints)\n",
        "  print(f\"Shape of altered_brightness_train_images: {np.shape(altered_brightness_train_images)}\")\n",
        "  print(f\"Shape of altered_brightness_train_keypoints: {np.shape(altered_brightness_train_keypoints)}\")\n",
        "\n",
        "\n",
        "  full_train_images = np.concatenate((full_train_images, altered_brightness_train_images))\n",
        "  full_train_keypoints = np.concatenate((full_train_keypoints, altered_brightness_train_keypoints))\n",
        "\n",
        "  fig, axis = plt.subplots()\n",
        "  plot_sample(altered_brightness_train_images[sample_image_index], altered_brightness_train_keypoints[sample_image_index], axis, \"Increased Brightness\") \n",
        "\n",
        "  fig, axis = plt.subplots()\n",
        "  image_plot =  altered_brightness_train_images[len(altered_brightness_train_images)//2+sample_image_index]\n",
        "  keypoints_plot = altered_brightness_train_keypoints[len(altered_brightness_train_images)//2+sample_image_index]\n",
        "  plot_sample(image_plot, keypoints_plot, axis, \"Decreased Brightness\") \n",
        "\n",
        "  print(full_train_images.shape)\n",
        "  print(full_train_keypoints.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WuiMlKpdPXj"
      },
      "outputs": [],
      "source": [
        "def shift_images(images, keypoints, pixel_shifts):\n",
        "    shifted_images = []\n",
        "    shifted_keypoints = []\n",
        "    for shift in pixel_shifts:    \n",
        "        for (shift_x,shift_y) in [(-shift,-shift),(-shift,shift),(shift,-shift),(shift,shift)]:\n",
        "            matrix = np.float32([[1,0,shift_x],[0,1,shift_y]])\n",
        "            \n",
        "            for image, keypoint in zip(images, keypoints):\n",
        "                shifted_image = cv2.warpAffine(image, matrix, (img_height,img_width), flags=cv2.INTER_CUBIC)\n",
        "                shifted_keypoint = np.array([(point+shift_x) if idx%2==0 else (point+shift_y) for idx, point in enumerate(keypoint)])\n",
        "                \n",
        "                if np.all(0.0<shifted_keypoint) and np.all(shifted_keypoint<img_height):\n",
        "                    shifted_images.append(shifted_image.reshape(img_height,img_width,1))\n",
        "                    shifted_keypoints.append(shifted_keypoint)\n",
        "                    \n",
        "    shifted_keypoints = np.clip(shifted_keypoints,0.0,img_height)\n",
        "    return shifted_images, shifted_keypoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URLK2qYVdW54"
      },
      "outputs": [],
      "source": [
        "shift_augmentation = True\n",
        "pixel_shifts = [12] \n",
        "\n",
        "if shift_augmentation:\n",
        "  shifted_train_images, shifted_train_keypoints = shift_images(train_images, train_keypoints, pixel_shifts)\n",
        "  print(f\"Shape of shifted_train_images: {np.shape(shifted_train_images)}\")\n",
        "  print(f\"Shape of shifted_train_keypoints: {np.shape(shifted_train_keypoints)}\")\n",
        "  \n",
        "  full_train_images = np.concatenate((full_train_images, shifted_train_images))\n",
        "  full_train_keypoints = np.concatenate((full_train_keypoints, shifted_train_keypoints))\n",
        "\n",
        "  fig, axis = plt.subplots()\n",
        "  plot_sample(shifted_train_images[sample_image_index], shifted_train_keypoints[sample_image_index], axis, \"Shift Augmentation\")\n",
        "  \n",
        "  print(full_train_images.shape)\n",
        "  print(full_train_keypoints.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cqftvfudcs0"
      },
      "outputs": [],
      "source": [
        "def add_noise(images):\n",
        "  noisy_images = []\n",
        "  for image in images:\n",
        "      noisy_image = cv2.add(image, 0.008*np.random.normal(img_height, img_width, 1))\n",
        "      noisy_images.append(noisy_image.reshape(img_height, img_width, 1))\n",
        "      \n",
        "  return noisy_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsjzP0upd7Ee"
      },
      "outputs": [],
      "source": [
        "random_noise_augmentation = True\n",
        "\n",
        "if random_noise_augmentation:\n",
        "  noisy_train_images = add_noise(train_images)\n",
        "  \n",
        "  print(f\"Shape of noisy_train_images: {np.shape(noisy_train_images)}\")\n",
        "  \n",
        "  full_train_images = np.concatenate((full_train_images, noisy_train_images))\n",
        "  full_train_keypoints = np.concatenate((full_train_keypoints, train_keypoints))\n",
        "  \n",
        "  fig, axis = plt.subplots()\n",
        "  plot_sample(noisy_train_images[sample_image_index], train_keypoints[sample_image_index], axis, \"Random Noise Augmentation\")\n",
        "  \n",
        "  print(full_train_images.shape)\n",
        "  print(full_train_keypoints.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BXDddyienhg"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(20,8))\n",
        "for i in range(10):\n",
        "    axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])\n",
        "    plot_sample(train_images[i], train_keypoints[i], axis, \"\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lh76KQ8If70K"
      },
      "outputs": [],
      "source": [
        "if horizontal_flip:\n",
        "    print(\"Horizontal Flip Augmentation: \")\n",
        "    fig = plt.figure(figsize=(20,8))\n",
        "    for i in range(10):\n",
        "        axis = fig.add_subplot(2,5,i+1,xticks=[],yticks=[])\n",
        "        plot_sample(flipped_train_images[i], flipped_train_keypoints[i], axis, \"\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3cV1p5kgCMf"
      },
      "outputs": [],
      "source": [
        "if rotation_augmentation:\n",
        "    print(\"Rotation Augmentation: \")\n",
        "    fig = plt.figure(figsize=(20,8))\n",
        "    for i in range(10):\n",
        "        axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])\n",
        "        plot_sample(rotated_train_images[i], rotated_train_keypoints_features[i], axis, \"\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2mfyCXMgU_4"
      },
      "outputs": [],
      "source": [
        "if brightness_augmentation:\n",
        "    print(\"Brightness Augmentation: \")\n",
        "    fig = plt.figure(figsize=(20,8))\n",
        "    for i in range(10):\n",
        "        axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])\n",
        "        plot_sample(altered_brightness_train_images[i], altered_brightness_train_keypoints[i], axis, \"\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-4umb6jga6x"
      },
      "outputs": [],
      "source": [
        "if shift_augmentation:\n",
        "    print(\"Shift Augmentation: \")\n",
        "    fig = plt.figure(figsize=(20,8))\n",
        "    for i in range(10):\n",
        "        axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])\n",
        "        plot_sample(shifted_train_images[i], shifted_train_keypoints[i], axis, \"\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GATjfgN4gj2e"
      },
      "outputs": [],
      "source": [
        "if random_noise_augmentation:\n",
        "    print(\"Random Noise Augmentation: \")\n",
        "    fig = plt.figure(figsize=(20,8))\n",
        "    for i in range(10):\n",
        "        axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])\n",
        "        plot_sample(noisy_train_images[i], train_keypoints[i], axis, \"\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxzVbvLWg4xX"
      },
      "outputs": [],
      "source": [
        "full_train_images, full_train_keypoints = shuffle(full_train_images, full_train_keypoints, random_state=0)\n",
        "print(full_train_images.shape)\n",
        "print(full_train_keypoints.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11r6DMxW5Vm6"
      },
      "source": [
        "#DATA PRE-PROCESSING DONE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edzVJdkZPiVy"
      },
      "source": [
        "#CREATE AND TRAIN MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jTaPj1OwLowC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2e3003d-9ddf-4d11-8924-80311bf6fc4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dm-haiku\n",
            "  Downloading dm_haiku-0.0.6-py3-none-any.whl (309 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 17.2 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20 kB 21.9 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30 kB 27.3 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51 kB 29.8 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 61 kB 32.2 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71 kB 33.0 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 81 kB 25.0 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 92 kB 26.4 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 102 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 112 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 122 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 133 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 143 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 153 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 163 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 174 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 184 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 194 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 204 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 215 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 225 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 235 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 245 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 256 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 266 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 276 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 286 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 296 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 307 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 309 kB 28.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from dm-haiku) (0.8.9)\n",
            "Collecting jmp>=0.0.2\n",
            "  Downloading jmp-0.0.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from dm-haiku) (1.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from dm-haiku) (4.1.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from dm-haiku) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.7.1->dm-haiku) (1.15.0)\n",
            "Installing collected packages: jmp, dm-haiku\n",
            "Successfully installed dm-haiku-0.0.6 jmp-0.0.2\n",
            "Collecting optax\n",
            "  Downloading optax-0.1.2-py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 24.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.7/dist-packages (from optax) (0.3.4)\n",
            "Collecting chex>=0.0.4\n",
            "  Downloading chex-0.1.3-py3-none-any.whl (72 kB)\n",
            "\u001b[K     |████████████████████████████████| 72 kB 572 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from optax) (1.21.6)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.7/dist-packages (from optax) (0.3.2+cuda11.cudnn805)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.7/dist-packages (from optax) (4.1.1)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from optax) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.7.1->optax) (1.15.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.1.7)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.11.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.1.55->optax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from jax>=0.1.55->optax) (1.4.1)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib>=0.1.37->optax) (2.0)\n",
            "Installing collected packages: chex, optax\n",
            "Successfully installed chex-0.1.3 optax-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -U dm-haiku\n",
        "!pip install optax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1AVZt-nV7a7F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b8989b5-3c50-4616-fc84-5bb87d298121"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Haiku Version: 0.0.6\n",
            "Jax Version: 0.3.4\n",
            "Optax Version: 0.1.2\n"
          ]
        }
      ],
      "source": [
        "import haiku as hk\n",
        "import jax, jax.numpy as jnp\n",
        "from jax import value_and_grad\n",
        "import optax\n",
        "\n",
        "from jax.example_libraries import stax, optimizers\n",
        "\n",
        "print(f\"Haiku Version: {hk.__version__}\")\n",
        "print(f\"Jax Version: {jax.__version__}\")\n",
        "print(f\"Optax Version: {optax.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Za3AiiMeQcA9"
      },
      "outputs": [],
      "source": [
        "full_train_images, full_train_keypoints, test_images, test_keypoints = jnp.array(full_train_images, dtype=np.float32),\\\n",
        "                                                                       jnp.array(full_train_keypoints, dtype=np.float32),\\\n",
        "                                                                       jnp.array(test_images, dtype=np.float32),\\\n",
        "                                                                       jnp.array(test_keypoints, dtype=np.float32)\n",
        "                                                                    \n",
        "class Convolution_4(hk.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__(name = \"CNN\")\n",
        "    self.Conv1 = hk.Conv2D(output_channels=32, kernel_shape=(3,3), padding=\"SAME\")\n",
        "    self.Conv2 = hk.Conv2D(output_channels=16, kernel_shape=(3,3), padding=\"SAME\")\n",
        "    self.flat = hk.Flatten()\n",
        "    self.max_pool = hk.max_pool(value=full_train_images, window_shape=(2,2), padding=\"SAME\")\n",
        "    self.dense = stax.Dense((16))\n",
        "\n",
        "  def __call__(self, x):\n",
        "\n",
        "    x = self.Conv1(x)\n",
        "    x = self.max_pool(x)\n",
        "    x = self.Conv2(x)\n",
        "    x = self.max_pool(x)\n",
        "    x = self.flat(x)\n",
        "    x = self.dense(x)\n",
        "    x = jax.nn.softmax(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "def ConvNet(x):\n",
        "  cnn = Convolution_4()\n",
        "  return cnn(x)\n",
        "\n",
        "def MSE(weights, input_data, actual):\n",
        "  preds = model.apply(weights, rng, input_data)\n",
        "  preds = preds.squeeze()\n",
        "  return jnp.power(actual - preds, 2).mean()\n",
        "\n",
        "def update(weights, o_grad):\n",
        "  return weights - lr * o_grad"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"HyperParameters\"\"\"\n",
        "model = hk.transform(ConvNet)\n",
        "rng = jax.random.PRNGKey(42)## Initializes model with same weights each time.\n",
        "\n",
        "params = model.init(rng, full_train_images.shape)\n",
        "epochs = 100\n",
        "batch_size = 32\n",
        "lr = jnp.array(0.01)"
      ],
      "metadata": {
        "id": "C6sHYNFY2UcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"training loop using batches of data\"\"\"\n",
        "\n",
        "for i in range(1, epochs+1):\n",
        "    batches = jnp.arange((full_train_images.shape[0]//batch_size)+1) ### Batch Indices\n",
        "\n",
        "    losses = [] ## Record loss of each batch\n",
        "    for batch in batches:\n",
        "        if batch != batches[-1]:\n",
        "            start, end = int(batch*batch_size), int(batch*batch_size+batch_size)\n",
        "        else:\n",
        "            start, end = int(batch*batch_size), None\n",
        "\n",
        "        X_batch, Y_batch = full_train_images[start:end], test_images[start:end] ## Single batch of data\n",
        "\n",
        "        loss, param_grads = value_and_grad(MSE)(params, X_batch, Y_batch)\n",
        "        params = jax.tree_map(update, params, param_grads) ## Update Params\n",
        "        losses.append(loss) ## Record Loss\n",
        "\n",
        "    if i % 20 == 0: ## Print MSE every 20 epochs\n",
        "        print(\"MSE : {:.2f}\".format(jnp.array(losses).mean()))"
      ],
      "metadata": {
        "id": "-Px0QWa52O6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MakePredictions(weights, input_data, batch_size=32):\n",
        "    batches = jnp.arange((input_data.shape[0]//batch_size)+1) ### Batch Indices\n",
        "\n",
        "    preds = []\n",
        "    for batch in batches:\n",
        "        if batch != batches[-1]:\n",
        "            start, end = int(batch*batch_size), int(batch*batch_size+batch_size)\n",
        "        else:\n",
        "            start, end = int(batch*batch_size), None\n",
        "\n",
        "        X_batch = input_data[start:end]\n",
        "\n",
        "        preds.append(model.apply(weights, rng, X_batch))\n",
        "\n",
        "    return preds"
      ],
      "metadata": {
        "id": "9LONV0ZH2ZBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Make Predictions in batches\"\"\"\n",
        "train_preds = MakePredictions(params, full_train_images, 32)\n",
        "train_preds = jnp.concatenate(train_preds).squeeze()\n",
        "\n",
        "test_preds = MakePredictions(params, test_images, 32)\n",
        "test_preds = jnp.concatenate(test_preds).squeeze()"
      ],
      "metadata": {
        "id": "0qqFkUPV6oou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Evaluate Performance\"\"\"\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "print(f\"TEST MSE: {MSE(params, test_images, test_keypoints)}\")\n",
        "print(f\"TRAIN MSE: {MSE(params, full_train_images, test_images)}\")\n",
        "\n",
        "\n",
        "print(f\"Test  R^2 Score : {r2_score(test_preds.squeeze(), test_images)}\")\n",
        "print(f\"Train R^2 Score : {(r2_score(train_preds.squeeze(), full_train_images))}\")"
      ],
      "metadata": {
        "id": "LTfx0srC2vv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"show images\"\"\"\n",
        "fig = plt.figure(figsize=(20,16))\n",
        "for i in range(20):\n",
        "    axis = fig.add_subplot(4, 5, i+1, xticks=[], yticks=[])\n",
        "    plot_sample(test_images[i], test_preds[i], axis, \"\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eD2HbPvH7ERy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Facial Keypoint Detection ( HAIKU / JAX ).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}